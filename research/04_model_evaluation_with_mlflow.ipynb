{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T04:28:46.214258Z",
     "start_time": "2025-03-20T04:28:46.205265Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Applications/AI/GAN/research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e50d5d5b0348681b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T04:28:46.220487Z",
     "start_time": "2025-03-20T04:28:46.215905Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Applications/AI/GAN'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8778ccafacf50ddc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T04:29:24.817546Z",
     "start_time": "2025-03-20T04:28:46.221450Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/AI/GAN/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-20 11:28:50,714: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2025-03-20 11:28:50,717: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-03-20 11:28:50,718: INFO: common: created directory at: artifacts]\n",
      "[2025-03-20 11:28:50,719: INFO: common: created directory at: artifacts/evaluation]\n",
      "[2025-03-20 11:28:50,719: INFO: 4243899433: Loading GAN models]\n",
      "[2025-03-20 11:28:51,229: INFO: 4243899433: Loading test data]\n",
      "[2025-03-20 11:28:51,397: INFO: 4243899433: Generating Monet-style samples]\n",
      "[2025-03-20 11:28:54,356: INFO: 4243899433: Calculating FID score]\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "[2025-03-20 11:29:23,987: INFO: 4243899433: FID Score: 458.2857848227136]\n",
      "[2025-03-20 11:29:23,992: INFO: 4243899433: Saving generated images]\n",
      "[2025-03-20 11:29:24,002: INFO: common: created directory at: artifacts/evaluation/generated_samples]\n",
      "[2025-03-20 11:29:24,811: INFO: 4243899433: Metrics saved to artifacts/evaluation/metrics.json]\n",
      "[2025-03-20 11:29:24,812: INFO: 4243899433: Attempting to log to MLflow]\n",
      "[2025-03-20 11:29:24,813: WARNING: 4243899433: MLflow tracking URI scheme 'file' not supported for artifact logging]\n",
      "[2025-03-20 11:29:24,814: INFO: 4243899433: To use MLflow, configure a valid tracking server with http/https]\n",
      "[2025-03-20 11:29:24,814: INFO: 4243899433: Evaluation completed successfully! Metrics: {'fid_score': 458.2857848227136}]\n"
     ]
    }
   ],
   "source": [
    "# GAN Model Evaluation: entity\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class GANEvaluationConfig:\n",
    "    generator_model_path: Path\n",
    "    discriminator_model_path: Path\n",
    "    test_data_path: Path\n",
    "    evaluation_output_path: Path\n",
    "    all_params: dict\n",
    "    mlflow_uri: str\n",
    "    params_image_size: list\n",
    "    params_batch_size: int\n",
    "    num_samples: int\n",
    "    save_format: str\n",
    "\n",
    "# GAN Model Evaluation: config manager\n",
    "from src.GAN.constants import *\n",
    "from src.GAN.utils.common import read_yaml, create_directories, save_json\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_gan_evaluation_config(self) -> GANEvaluationConfig:\n",
    "        eval_config = GANEvaluationConfig(\n",
    "            generator_model_path=Path(self.config.gan_training.trained_generator_path),\n",
    "            discriminator_model_path=Path(self.config.gan_training.trained_discriminator_path),\n",
    "            test_data_path=Path(self.config.data_ingestion.unzip_dir),\n",
    "            evaluation_output_path=Path(self.config.evaluation.output_path),\n",
    "            mlflow_uri=self.config.evaluation.mlflow_uri,\n",
    "            all_params=self.params,\n",
    "            params_image_size=self.params.IMAGE_SIZE,\n",
    "            params_batch_size=self.params.BATCH_SIZE,\n",
    "            num_samples=self.params.EVAL_SAMPLES,\n",
    "            save_format=self.params.SAVE_FORMAT\n",
    "        )\n",
    "        return eval_config\n",
    "\n",
    "# GAN Model Evaluation: component\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from urllib.parse import urlparse\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from scipy.linalg import sqrtm\n",
    "from tqdm import tqdm\n",
    "from src.GAN import logger\n",
    "\n",
    "class GANEvaluation:\n",
    "    def __init__(self, config: GANEvaluationConfig):\n",
    "        self.config = config\n",
    "        create_directories([self.config.evaluation_output_path])\n",
    "        \n",
    "    def load_models(self):\n",
    "        \"\"\"Load generator and discriminator models\"\"\"\n",
    "        logger.info(\"Loading GAN models\")\n",
    "        self.generator = tf.keras.models.load_model(self.config.generator_model_path)\n",
    "        self.discriminator = tf.keras.models.load_model(self.config.discriminator_model_path)\n",
    "    \n",
    "    def load_test_data(self):\n",
    "        \"\"\"Load test images for evaluation\"\"\"\n",
    "        logger.info(\"Loading test data\")\n",
    "        \n",
    "        # Find paths containing photos and Monet images\n",
    "        photo_dir = self.find_directory_with_images(self.config.test_data_path, \"photo\")\n",
    "        monet_dir = self.find_directory_with_images(self.config.test_data_path, \"monet\")\n",
    "        \n",
    "        if not photo_dir or not monet_dir:\n",
    "            raise ValueError(\"Could not find photo and Monet directories\")\n",
    "        \n",
    "        # Load a batch of test photos\n",
    "        photo_files = self.get_image_files(photo_dir)\n",
    "        self.test_photos = self.load_images(\n",
    "            photo_files[:self.config.num_samples],\n",
    "            self.config.params_image_size[:2]\n",
    "        )\n",
    "        \n",
    "        # Load real Monet paintings for comparison\n",
    "        monet_files = self.get_image_files(monet_dir)\n",
    "        self.real_monet = self.load_images(\n",
    "            monet_files[:self.config.num_samples],\n",
    "            self.config.params_image_size[:2]\n",
    "        )\n",
    "    \n",
    "    def find_directory_with_images(self, root_dir, name_contains):\n",
    "        \"\"\"Find a directory that contains images and has 'name_contains' in its path\"\"\"\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            image_files = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            if image_files and name_contains.lower() in root.lower():\n",
    "                return root\n",
    "        return None\n",
    "    \n",
    "    def get_image_files(self, directory):\n",
    "        \"\"\"Get all image files in a directory\"\"\"\n",
    "        extensions = ['.jpg', '.jpeg', '.png']\n",
    "        files = []\n",
    "        for ext in extensions:\n",
    "            files.extend([os.path.join(directory, f) for f in os.listdir(directory)\n",
    "                         if f.lower().endswith(ext)])\n",
    "        return files\n",
    "    \n",
    "    def load_images(self, file_paths, image_size):\n",
    "        \"\"\"Load and preprocess images from file paths\"\"\"\n",
    "        images = []\n",
    "        for path in file_paths:\n",
    "            img = tf.io.read_file(path)\n",
    "            img = tf.io.decode_jpeg(img, channels=3)\n",
    "            img = tf.image.resize(img, image_size)\n",
    "            img = (tf.cast(img, tf.float32) / 127.5) - 1  # Normalize to [-1, 1]\n",
    "            images.append(img)\n",
    "        \n",
    "        if images:\n",
    "            return tf.stack(images)\n",
    "        return None\n",
    "    \n",
    "    def generate_samples(self):\n",
    "        \"\"\"Generate Monet-style images from test photos\"\"\"\n",
    "        logger.info(\"Generating Monet-style samples\")\n",
    "        self.generated_images = self.generator(self.test_photos, training=False)\n",
    "        return self.generated_images\n",
    "    \n",
    "    def calculate_fid(self, real_images, generated_images):\n",
    "        \"\"\"Calculate Fr√©chet Inception Distance between real and generated images\"\"\"\n",
    "        logger.info(\"Calculating FID score\")\n",
    "        \n",
    "        # Load InceptionV3 model\n",
    "        inception_model = InceptionV3(include_top=False, pooling='avg', input_shape=(299, 299, 3))\n",
    "        \n",
    "        # Resize and preprocess images for InceptionV3\n",
    "        def preprocess_for_inception(images):\n",
    "            images = tf.image.resize(images, (299, 299))\n",
    "            images = (images + 1) * 127.5  # Convert from [-1, 1] to [0, 255]\n",
    "            images = preprocess_input(images)  # Inception preprocessing\n",
    "            return images\n",
    "        \n",
    "        real_preprocessed = preprocess_for_inception(real_images)\n",
    "        generated_preprocessed = preprocess_for_inception(generated_images)\n",
    "        \n",
    "        # Extract features\n",
    "        real_features = inception_model.predict(real_preprocessed)\n",
    "        generated_features = inception_model.predict(generated_preprocessed)\n",
    "        \n",
    "        # Calculate mean and covariance\n",
    "        mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n",
    "        mu2, sigma2 = generated_features.mean(axis=0), np.cov(generated_features, rowvar=False)\n",
    "        \n",
    "        # Calculate square root of product of covariances\n",
    "        ssdiff = np.sum((mu1 - mu2) ** 2.0)\n",
    "        covmean = sqrtm(sigma1.dot(sigma2))\n",
    "        \n",
    "        # Check and correct imaginary numbers from sqrt\n",
    "        if np.iscomplexobj(covmean):\n",
    "            covmean = covmean.real\n",
    "        \n",
    "        # Calculate FID\n",
    "        fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "        return fid\n",
    "    \n",
    "    def save_generated_images(self):\n",
    "        \"\"\"Save generated images for visual inspection\"\"\"\n",
    "        logger.info(\"Saving generated images\")\n",
    "        \n",
    "        save_dir = os.path.join(self.config.evaluation_output_path, \"generated_samples\")\n",
    "        create_directories([save_dir])\n",
    "        \n",
    "        # Create a grid of original vs generated images\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        \n",
    "        for i in range(min(10, len(self.test_photos))):\n",
    "            # Display original photo\n",
    "            plt.subplot(2, 10, i + 1)\n",
    "            plt.title(\"Original\")\n",
    "            photo = (self.test_photos[i] + 1) * 0.5  # Denormalize\n",
    "            plt.imshow(photo)\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Display generated Monet-style image\n",
    "            plt.subplot(2, 10, i + 11)\n",
    "            plt.title(\"Generated\")\n",
    "            generated = (self.generated_images[i] + 1) * 0.5  # Denormalize\n",
    "            plt.imshow(generated)\n",
    "            plt.axis('off')\n",
    "        \n",
    "        comparison_path = os.path.join(save_dir, \"comparison.png\")\n",
    "        plt.savefig(comparison_path)\n",
    "        plt.close()\n",
    "        \n",
    "        # Save individual generated images\n",
    "        for i, img in enumerate(self.generated_images):\n",
    "            img = (img + 1) * 127.5  # Convert from [-1, 1] to [0, 255]\n",
    "            img = tf.cast(img, tf.uint8)\n",
    "            img_path = os.path.join(save_dir, f\"generated_{i}.{self.config.save_format}\")\n",
    "            \n",
    "            if self.config.save_format == 'png':\n",
    "                tf.io.write_file(img_path, tf.io.encode_png(img))\n",
    "            else:\n",
    "                tf.io.write_file(img_path, tf.io.encode_jpeg(img))\n",
    "        \n",
    "        return comparison_path, save_dir\n",
    "    \n",
    "    # Add this method to your GANEvaluation class\n",
    "    def save_metrics_json(self, metrics_dict, filepath):\n",
    "        \"\"\"Save metrics to JSON file without using the problematic save_json function\"\"\"\n",
    "        import json\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(metrics_dict, f, indent=4)\n",
    "        logger.info(f\"Metrics saved to {filepath}\")\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate GAN performance\"\"\"\n",
    "        self.load_models()\n",
    "        self.load_test_data()\n",
    "        self.generate_samples()\n",
    "        \n",
    "        # Calculate FID score\n",
    "        try:\n",
    "            self.fid_score = self.calculate_fid(self.real_monet, self.generated_images)\n",
    "            logger.info(f\"FID Score: {self.fid_score}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating FID score: {e}\")\n",
    "            self.fid_score = None\n",
    "        \n",
    "        # Save generated images\n",
    "        self.comparison_path, self.samples_dir = self.save_generated_images()\n",
    "        \n",
    "        # Save scores directly using json module\n",
    "        import json\n",
    "        scores = {\"fid_score\": float(self.fid_score) if self.fid_score is not None else None}\n",
    "        metrics_path = os.path.join(self.config.evaluation_output_path, \"metrics.json\")\n",
    "        os.makedirs(os.path.dirname(metrics_path), exist_ok=True)\n",
    "        with open(metrics_path, 'w') as f:\n",
    "            json.dump(scores, f, indent=4)\n",
    "        logger.info(f\"Metrics saved to {metrics_path}\")\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def log_into_mlflow(self):\n",
    "        \"\"\"Log evaluation metrics and artifacts to MLflow with better error handling\"\"\"\n",
    "        logger.info(\"Attempting to log to MLflow\")\n",
    "        \n",
    "        # Skip MLflow logging if no URI is configured\n",
    "        if not self.config.mlflow_uri:\n",
    "            logger.info(\"No MLflow URI configured, skipping MLflow logging\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "            tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "            \n",
    "            # Only proceed with MLflow logging if proper URI is configured\n",
    "            if tracking_url_type_store not in [\"http\", \"https\"]:\n",
    "                logger.warning(f\"MLflow tracking URI scheme '{tracking_url_type_store}' not supported for artifact logging\")\n",
    "                logger.info(\"To use MLflow, configure a valid tracking server with http/https\")\n",
    "                return\n",
    "            \n",
    "            with mlflow.start_run():\n",
    "                # Log parameters\n",
    "                mlflow.log_params(self.config.all_params)\n",
    "                \n",
    "                # Log metrics\n",
    "                if self.fid_score is not None:\n",
    "                    mlflow.log_metric(\"fid_score\", self.fid_score)\n",
    "                \n",
    "                # Log artifacts and models\n",
    "                mlflow.log_artifact(self.comparison_path)\n",
    "                mlflow.keras.log_model(self.generator, \"generator_model\")\n",
    "                mlflow.keras.log_model(self.discriminator, \"discriminator_model\")\n",
    "                \n",
    "                logger.info(\"Successfully logged to MLflow\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error logging to MLflow: {e}\")\n",
    "            logger.info(\"Evaluation results are still saved locally\")\n",
    "\n",
    "# GAN Model Evaluation: pipeline\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    eval_config = config.get_gan_evaluation_config()\n",
    "    evaluation = GANEvaluation(eval_config)\n",
    "    \n",
    "    scores = evaluation.evaluate()\n",
    "    \n",
    "    # Try MLflow logging but continue if it fails\n",
    "    try:\n",
    "        evaluation.log_into_mlflow()\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"MLflow logging failed: {e}\")\n",
    "    \n",
    "    logger.info(f\"Evaluation completed successfully! Metrics: {scores}\")\n",
    "except Exception as e:\n",
    "    logger.exception(f\"Error during GAN evaluation: {e}\")\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
